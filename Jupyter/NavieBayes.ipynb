{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset():\n",
    "    postingList = [\n",
    "        [\"my\", \"dog\", \"has\", \"flea\", \"problems\", \"help\", \"please\"],\n",
    "        [\"maybe\", \"not\", \"take\", \"him\", \"to\", \"dog\", \"park\", \"stupid\"],\n",
    "        [\"my\", \"dalmation\", \"is\", \"so\", \"cute\", \"I\", \"love\", \"him\"],\n",
    "        [\"stop\", \"posting\", \"stupid\", \"worthless\", \"garbage\"],\n",
    "        [\"mr\", \"licks\", \"ate\", \"my\", \"steak\", \"how\", \"to\", \"stop\", \"him\"],\n",
    "        [\"quit\", \"buying\", \"worthless\", \"dog\", \"food\", \"stupid\"]]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]\n",
    "    return postingList, classVec    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataset):\n",
    "    Vocavset = set()\n",
    "    for line in dataset:\n",
    "        for word in line:\n",
    "            Vocavset.add(word)\n",
    "    return list(Vocavset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetOfWord2Vec(vocablist, inputset):\n",
    "    returnvec = [0]*len(vocablist)\n",
    "    for word in inputset:\n",
    "        if word in vocablist:\n",
    "            returnvec[vocablist.index(word)] = 1\n",
    "        else:\n",
    "            print (\"the word: %s is not in vocabulary !\" %word)\n",
    "    return returnvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "postingList, classVec = loadDataset()\n",
    "vocablist = createVocabList(postingList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnvec = SetOfWord2Vec(vocablist, postingList[0])\n",
    "returnvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMatrix = []\n",
    "for postingdoc in postingList:\n",
    "    trainMatrix.append(SetOfWord2Vec(vocablist, postingdoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, traincategory):\n",
    "    num_traindoc = len(trainMatrix)\n",
    "    nums_words = len(trainMatrix[0])\n",
    "    pAbusive = sum(traincategory)/float(num_traindoc)\n",
    "    p0_num, p1_num = np.ones(nums_words), np.ones(nums_words)\n",
    "    p0_Denom, p1_Denom = 2.0, 2.0\n",
    "    for i in range(num_traindoc):\n",
    "        if traincategory[i] == 1:\n",
    "            p1_num += trainMatrix[i]\n",
    "            p1_Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0_num += trainMatrix[i]\n",
    "            p0_Denom += sum(trainMatrix[i])\n",
    "    # p1vec = p1_num/p1_Denom\n",
    "    # p0vec = p0_num/p0_Denom\n",
    "    p1vec = np.log(p1_num/p1_Denom)\n",
    "    p0vec = np.log(p0_num/p0_Denom)\n",
    "    return p0vec, p1vec, pAbusive\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traincategory = classVec\n",
    "p0vec, p1vec, pAbusive = trainNB0(trainMatrix, traincategory)\n",
    "pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.94591015, -2.35137526, -3.04452244, -2.35137526, -2.35137526,\n",
       "       -3.04452244, -3.04452244, -3.04452244, -3.04452244, -2.35137526,\n",
       "       -3.04452244, -2.35137526, -3.04452244, -3.04452244, -3.04452244,\n",
       "       -2.35137526, -2.35137526, -3.04452244, -3.04452244, -3.04452244,\n",
       "       -1.65822808, -2.35137526, -2.35137526, -1.94591015, -2.35137526,\n",
       "       -3.04452244, -3.04452244, -2.35137526, -3.04452244, -3.04452244,\n",
       "       -2.35137526, -3.04452244])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.25809654, -3.25809654, -2.56494936, -3.25809654, -2.56494936,\n",
       "       -2.56494936, -2.56494936, -2.56494936, -2.56494936, -3.25809654,\n",
       "       -2.56494936, -2.15948425, -2.56494936, -2.56494936, -2.56494936,\n",
       "       -2.56494936, -3.25809654, -2.56494936, -2.56494936, -2.56494936,\n",
       "       -3.25809654, -3.25809654, -3.25809654, -2.56494936, -3.25809654,\n",
       "       -2.56494936, -2.56494936, -3.25809654, -2.56494936, -2.56494936,\n",
       "       -3.25809654, -1.87180218])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2classify, p0vec, p1vec, pclass1):\n",
    "    p1 = sum(vec2classify*p1vec) + np.log(pclass1)\n",
    "    p0 = sum(vec2classify*p0vec) + np.log(1-pclass1)\n",
    "    return 1 if p1 > p0 else 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB(testEntry):\n",
    "    postingList, classVec = loadDataset()\n",
    "    vocablist = createVocabList(postingList)\n",
    "    trainMatrix = []\n",
    "    for postingdoc in postingList:\n",
    "        trainMatrix.append(SetOfWord2Vec(vocablist, postingdoc))\n",
    "    p0vec, p1vec, pAbusive = trainNB0(np.array(trainMatrix), np.array(classVec))\n",
    "    thisdoc = np.array(SetOfWord2Vec(vocablist, testEntry))\n",
    "    print(testEntry, \"classify as:\", classifyNB(thisdoc, p0vec, p1vec, pAbusive))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classify as: 0\n",
      "['stupid', 'garbage'] classify as: 1\n"
     ]
    }
   ],
   "source": [
    "testEntry = [\"love\", \"my\", \"dalmation\"]\n",
    "testingNB(testEntry)\n",
    "testEntry = [\"stupid\", \"garbage\"]\n",
    "testingNB(testEntry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BagOfWord2Vec(vocablist, inputset):\n",
    "    returnvec = [0]*len(vocablist)\n",
    "    for word in inputset:\n",
    "        if word in vocablist:\n",
    "            returnvec[vocablist.index(word)] += 1\n",
    "        else:\n",
    "            print (\"the word: %s is not in vocabulary !\" %word)\n",
    "    return returnvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M.L.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon.']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysent = \"This book is the best book on Python or M.L. I have ever laid eyes upon.\"\n",
    "mysent1 = mysent.split(\" \")\n",
    "mysent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M',\n",
       " 'L',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon',\n",
       " '']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "regEX = re.compile('\\\\W+') #匹配非英文字母和数字\n",
    "listoftokens = regEX.split(mysent)\n",
    "listoftokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'python',\n",
       " 'or',\n",
       " 'm',\n",
       " 'l',\n",
       " 'i',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ListOfTokens = [token.lower() for token in listoftokens if len(token)>0]\n",
    "ListOfTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e213d5e9d4b9ef163e4a671f8b896840d73e50d9fb84158d652086491461306b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf2.2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
